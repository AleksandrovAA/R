---
title: "Упражнение № 5"
author: "Александрова Александра"
date: "21 03 2021"
output: html_document
---


## Вариант 2

1 Оценить стандартную ошибку модели для линейных регрессионных моделей из упражнения 4 (варианты ниже): а) со всеми объясняющими переменными; б) только с непрерывными объясняющими переменными:

 - методом проверочной выборки с долей обучающей 50%;

 - методом LOOCV;

 - k-кратной кросс-валидацией с k=5 и k=10.

Выбрать лучшую модель по минимуму ошибки. Все ли методы кросс-валидации сходятся на одной и той же модели?


2 Оценить стандартные ошибки параметров лучшей модели регрессии методом бутстрепа. Вывести график остатков лучшей модели. Сравнить с оценками стандартных ошибок параметров по МНК.


```{r setup, include=FALSE}

# загрузка пакетов
library('MASS')         # загружаем пакет
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN
library('boot')              # расчёт ошибки с кросс-валидацией

knitr::opts_chunk$set(echo = TRUE)
```


## Описание переменных

Набор данных Boston содержит переменные:

*crim* - уровень преступности на душу населения в разбивке по городам;

*indus* - доля акров не розничного бизнеса на один город;

*tax* - полная стоимость имущества-ставка налога на имущество за \$10 000;

*black* - 1000(Bk - 0.63)^2, где Bk-доля чернокожих по городам;

*chas* - Charles River (= 1, территория имеет водную границу; 0 территория не имеет водную границу);

Размерность обучающей выборки: n = 506 строк, p = 4 объясняющих переменных. Зависимая переменная – *crim*. Дискретная переменная - *chas*


### Метод перекрёстной проверки

Рассмотрим данные с характеристиками города Boston из пакета MASS. Скопируем таблицу во фрейм DF.boston для дальнейших манипуляций.


```{r}

my.seed <- 2

DF.boston <- subset(Boston, select = c(crim, tax, black, indus, chas))

#DF.boston <- Boston

head(DF.boston)

str(DF.boston) 

```


## Oписательные статистики по переменным

```{r}

summary(DF.boston)

```

В таблице данных 506 наблюдений и 4 переменных, среди которых есть непрерывные количественные и дискретные количественные и одна номинальная (name, название модели автомобиля, сохранено как фактор). В данном случае по функции summary() сложно определить реальные типы переменных, помогает table() от отдельных столбцов таблицы: если уникальных значений немного, перед нами фактор.


#### Количество цилиндров

```{r}

table(DF.boston$chas)

```


Построим графики разброса, показав факторы *chas* (число цилиндров)  цветом. Зависимой переменной модели является *crim*, её покажем в первой строке / столбце матричного графика. Во вторую строку / столбец поставим фактор.


```{r}

# переводим дискретные количественные переменные в факторы
DF.boston$chas <- as.factor(DF.boston$chas)

# графики разброса, цвет -- количество цилиндров
ggpairs(DF.boston[, c(1, 2, 5)], ggplot2::aes(color = chas))

ggpairs(DF.boston[, c(1, 3, 5)], ggplot2::aes(color = chas))

ggpairs(DF.boston[, c(1, 4, 5)], ggplot2::aes(color = chas))
```


#### Графики зависимости  crim ~ tax ,  crim ~ indus,  crim ~ black
```{r}

plot(DF.boston$tax , DF.boston$crim,
     xlab = 'tax ', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))

plot(DF.boston$indus, DF.boston$crim,
     xlab = 'indus', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))

plot(DF.boston$black, DF.boston$crim,
     xlab = 'black', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))

```


## Метод проверочной выборки

Он состоит в том, что мы отбираем одну тестовую выборку и будем считать на ней ошибку модели

```{r}
# общее число наблюдений
n <- nrow(DF.boston)

# доля обучающей выборки
train.percent <- 0.5

# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(1:n, n * train.percent)

# фактические значения Y на тестовой выборке
y.test.fact <- DF.boston$crim[-inTrain]

# рисуем разными цветами обучающую и тестовую
plot(DF.boston$tax [inTrain], DF.boston$crim[inTrain],
     xlab = 'tax ', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
points(DF.boston$tax [-inTrain], DF.boston$crim[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), 
       bg = rgb(1, 0, 0, alpha = 0.4))
legend('topright', 
       pch = c(16, 16), col = c('blue', 'red'), legend = c('test', 'train'))

plot(DF.boston$indus[inTrain], DF.boston$crim[inTrain],
     xlab = 'indus', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
points(DF.boston$indus[-inTrain], DF.boston$crim[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), 
       bg = rgb(1, 0, 0, alpha = 0.4))
legend('topright', 
       pch = c(16, 16), col = c('blue', 'red'), legend = c('test', 'train'))

plot(DF.boston$black[inTrain], DF.boston$crim[inTrain],
     xlab = 'black', ylab = 'crim', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
points(DF.boston$black[-inTrain], DF.boston$crim[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), 
       bg = rgb(1, 0, 0, alpha = 0.4))
legend('topright', 
       pch = c(16, 16), col = c('blue', 'red'), legend = c('test', 'train'))



DF.boston$Выборка <- 1
DF.boston$Выборка[inTrain] <- 2
DF.boston$Выборка <- as.factor(DF.boston$Выборка)
levels(DF.boston$Выборка) <- c('test','train')

ggplot(
  DF.boston, aes(x = chas, y = crim)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_jitter(aes(bg = Выборка),position = position_jitter(width = .1, height = 0),
  pch = 21, col = rgb(0, 0, 1, alpha = 0.4)
  )

```
Построим модели для проверки точности со всеми объясняющими переменными.


Вид моделей:

$$crim=f(tax  + indus + black + chas)$$
Линейная модель: 

$$crim=β_0+β_1⋅weihgt +β_2 indus + β_3 black + β_4 chas$$



```{r, warning=FALSE}

# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.1_1 <- lm(crim ~ tax  + indus + black + chas, subset = inTrain)

# подгонка линейной модели на обучающей выборке
fit.lm.1_1 <- lm(crim ~ tax  + indus + black + chas, 
               subset = inTrain)
# прогноз на тестовую
y.test.lm.1_1 <- predict(fit.lm.1_1, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.1_1 <- mean((y.test.fact - y.test.lm.1_1)^2)

# отсоединить таблицу с данными
detach(DF.boston)

# смотрим ошибку
MSE.lm.1_1

```


Строим квадратичную модель: 

$$crim = β_0 + β_1tax  + β_2 indus + β_3 black + β_4chas + β_5 tax ^2 + β_6 indus^2 + β_7 black^2 + β_8 chas^2$$


```{r}

# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.2_1 <- lm(crim ~ poly(tax , 2) + poly(indus, 2) + poly(black, 2) + chas, subset = inTrain)

# прогноз на тестовую
y.test.lm.2_1 <- predict(fit.lm.2_1, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.2_1 <- round(mean((y.test.fact - y.test.lm.2_1)^2), 2)

# отсоединить таблицу с данными

detach(DF.boston)

# смотрим ошибку
MSE.lm.2_1

```


## Строим кубическую модель: 

$$crim=β_0+β_1tax +β_1tax  + β_2 indus + β_3 black+ β_5 indus^2 + 

+β_6 black^2+ + β_4tax ^2  β_4tax ^2 + β_5 indus^2 + β_6 black^2+β_7⋅tax ^3+ β_8 indus^3 + β_9 black^3 + β_10 chas + β_11 chas^2 + β_12 chas^3$$


 Присоединить таблицу с данными: названия стоблцов будут доступны напрямую


```{r}

attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.3_1 <- lm(crim ~ poly(tax , 3)  + poly(indus, 3) + poly(black, 3) + chas, 
               subset = inTrain)

# прогноз на тестовую
y.test.lm.3_1 <- predict(fit.lm.3_1, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.3_1 <- round(mean((y.test.fact - y.test.lm.3_1)^2), 2)

# отсоединить таблицу с данными
detach(DF.boston)

# смотрим ошибку
MSE.lm.3_1

```


## Перекрёстная проверка по отдельным наблюдениям (LOOCV)

Это самый затратный в вычислительном плане метод, но и самый надёжный в плане оценки ошибки вне выборки. Попробуем применить его к линейной модели.


```{r}

# подгонка линейной модели на обучающей выборке
fit.glm_1 <- glm(crim ~ tax  + indus + black + chas, data = DF.boston)

# считаем LOOCV-ошибку
cv.err_1 <- cv.glm(DF.boston, fit.glm_1)

# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err_1$delta[1]

```


Теперь оценим точность полиномиальных моделей, меняя степень, в которой стоит регрессор.


```{r}

# вектор с LOOCV-ошибками
cv.err.loocv_1 <- rep(0, 5)
# имена элементов вектора
names(cv.err.loocv_1) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm_1 <- glm(crim ~ poly(tax , i)  + poly(indus, i) + poly(black, i) + chas, data = DF.boston)
  # расчёт ошибки
  cv.err.loocv_1[i] <- cv.glm(DF.boston, fit.glm_1)$delta[1]
}

# результат
cv.err.loocv_1

```






Построим модели для проверки точности только c непрерывными переменными.

Вид моделей:

$$crim=f(tax  + indus + black)$$


Линейная модель: 

$$crim=β_0+β_1⋅weihgt +β_2 indus +β_3 black$$



```{r, warning=FALSE}

# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.1 <- lm(crim ~ tax  + indus + black, subset = inTrain)

# подгонка линейной модели на обучающей выборке
fit.lm.1 <- lm(crim ~ tax  + indus + black, 
               subset = inTrain)
# прогноз на тестовую
y.test.lm.1 <- predict(fit.lm.1, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.1 <- mean((y.test.fact - y.test.lm.1)^2)

# отсоединить таблицу с данными
detach(DF.boston)

# смотрим ошибку
MSE.lm.1

```


Строим квадратичную модель: 

$$crim = β_0 + β_1tax  + β_2 indus + β_3 black + β_4tax ^2 + β_5 indus^2 + β_6 black^2$$


```{r}

# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.2 <- lm(crim ~ poly(tax , 2) + poly(indus, 2) + poly(black, 2), subset = inTrain)

# прогноз на тестовую
y.test.lm.2 <- predict(fit.lm.2, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.2 <- round(mean((y.test.fact - y.test.lm.2)^2), 2)

# отсоединить таблицу с данными

detach(DF.boston)

# смотрим ошибку
MSE.lm.2

```


## Строим кубическую модель: 

$$crim=β_0+β_1tax +β_1tax  + β_2 indus + β_3 black+ β_5 indus^2 + 

+β_6 black^2+ + β_4tax ^2  β_4tax ^2 + β_5 indus^2 + β_6 black^2+β_7⋅tax ^3+ β_8 indus^3 + β_9 black^3$$


 Присоединить таблицу с данными: названия стоблцов будут доступны напрямую


```{r}

attach(DF.boston)

# подгонка модели на обучающей выборке
fit.lm.3 <- lm(crim ~ poly(tax , 3)  + poly(indus, 3) + poly(black, 3), 
               subset = inTrain)

# прогноз на тестовую
y.test.lm.3 <- predict(fit.lm.3, DF.boston[-inTrain, ])

# считаем MSE на тестовой выборке
MSE.lm.3 <- round(mean((y.test.fact - y.test.lm.3)^2), 2)

# отсоединить таблицу с данными
detach(DF.boston)

# смотрим ошибку
MSE.lm.3

```


## Перекрёстная проверка по отдельным наблюдениям (LOOCV)

Это самый затратный в вычислительном плане метод, но и самый надёжный в плане оценки ошибки вне выборки. Попробуем применить его к линейной модели.


```{r}

# подгонка линейной модели на обучающей выборке
fit.glm <- glm(crim ~ tax  + indus + black, data = DF.boston)

# считаем LOOCV-ошибку
cv.err <- cv.glm(DF.boston, fit.glm)

# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err$delta[1]

```


Теперь оценим точность полиномиальных моделей, меняя степень, в которой стоит регрессор.


```{r}

# вектор с LOOCV-ошибками
cv.err.loocv <- rep(0, 5)
# имена элементов вектора
names(cv.err.loocv) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm <- glm(crim ~ poly(tax , i)  + poly(indus, i) + poly(black, i), data = DF.boston)
  # расчёт ошибки
  cv.err.loocv[i] <- cv.glm(DF.boston, fit.glm)$delta[1]
}

# результат
cv.err.loocv

```


## k-кратная перекрёстная проверка

K-кратная кросс-валидация – компромисс между методом проверочной выборки и LOOCV. Оценка ошибки вне выборки ближе к правде, по сравнению с проверочной выборкой, а объём вычислений меньше, чем при LOOCV. Проведём 10-ти кратную и 5-ти кратную кросс-валидацию моделей разных степеней.

# 5-ти кратная 

```{r}


# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 5-ти кратной кросс-валидации
cv.err.k.fold5 <- rep(0, 5)
# имена элементов вектора
names(cv.err.k.fold5) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm <- glm(crim ~ poly(tax , i) + poly(indus, i) + poly(black, i), data = DF.boston)
  # расчёт ошибки
  cv.err.k.fold5[i] <- cv.glm(DF.boston, fit.glm, K = 5)$delta[1]
}

# результат
cv.err.k.fold5

```


# 10-ти кратная

```{r}

# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 10-кратной кросс-валидации
cv.err.k.fold10 <- rep(0, 5)
# имена элементов вектора
names(cv.err.k.fold10) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm <- glm(crim ~ poly(tax , i) + poly(indus, i) + poly(black, i), data = DF.boston)
  # расчёт ошибки
  cv.err.k.fold10[i] <- cv.glm(DF.boston, fit.glm, K = 10)$delta[1]
}

# результат
cv.err.k.fold10

```

## для модели с фиктивной переменной

# 5-ти кратная 


```{r}


# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 5-ти кратной кросс-валидации
cv.err.k.fold5_1 <- rep(0, 5)
# имена элементов вектора
names(cv.err.k.fold5_1) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm_1 <- glm(crim ~ poly(tax , i) + poly(indus, i) + poly(black, i) + chas, data = DF.boston)
  # расчёт ошибки
  cv.err.k.fold5_1[i] <- cv.glm(DF.boston, fit.glm_1, K = 5)$delta[1]
}

# результат
cv.err.k.fold5_1

```

# 10-ти кратная

```{r}

# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 10-кратной кросс-валидации
cv.err.k.fold10_1 <- rep(0, 5)
# имена элементов вектора
names(cv.err.k.fold10_1) <- 1:5

# цикл по степеням полиномов
for (i in 1:5) {
  # оценка модели
  fit.glm_1 <- glm(crim ~ poly(tax , i) + poly(indus, i) + poly(black, i) + chas, data = DF.boston)
  # расчёт ошибки
  cv.err.k.fold10_1[i] <- cv.glm(DF.boston, fit.glm_1, K = 10)$delta[1]
}

# результат
cv.err.k.fold10_1

```

Объединим все ошибки в одну таблицу и отсортируем её по возрастанию MSE (с непрерывными) и MSE.1 (со всеми обяняющими переменными):


```{r}

# записываем все ошибки в таблицу
df.MSE <- data.frame(Модель = c('Линейная', 'Полином 2 степени',
                                'Полином 3 степени', 
                                rep(paste('Полином', 1:5, 'степени'), 3)), 
                     Проверка.точности = c(rep('Проверочная выборка 50%', 3),
                                           rep('LOOCV', 5), 
                                           rep('Кросс-валидация, k = 5', 5),
                                           rep('Кросс-валидация, k = 10', 5)),
                     MSE = round(c(MSE.lm.1, MSE.lm.2, MSE.lm.3, 
                                  cv.err.loocv, cv.err.k.fold10, cv.err.k.fold5), 2), 
                     MSE = round(c(MSE.lm.1_1, MSE.lm.2_1, MSE.lm.3_1, 
                                  cv.err.loocv_1, cv.err.k.fold10_1, cv.err.k.fold5_1), 2))

# все модели по возрастанию ошибки
df.MSE[order(df.MSE$MSE), ]

```


Опираясь на результаты расчётов с кросс-валидацией, можно заключить, что на самом деле ошибка вне выборки у линейной модели выше, чем показывала MSE на тестовой выборке. В целом, ошибка методом проверочной выборки размером 50% от числа наблюдений занижает MSE и, следовательно, завышает точность моделей. Та же ситуация наблюдается и у моделей со всеми обяъсняющими переменными.


# Бутстреп

## Точность оценки параметра регрессии

При построении модели регрессии проблемы в остатках приводят к неверной оценке ошибок параметров. Обойти эту проблему можно, применив для расчёта этих ошибок бутстреп.


```{r}

# Оценивание точности линейной регрессионной модели ----------------------------

# оценить стандартные ошибки параметров модели 
#  crim = beta_0 + beta_1 * horsepower с помощью бутстрепа,
#  сравнить с оценками ошибок по МНК

# функция для расчёта коэффициентов ПЛР по выборке из данных
boot.fn <- function(data, index){
  coef(lm(crim ~ tax  + indus + black, data = data, subset = index))
}
boot.fn(DF.boston, 1:n)

```


# применениe функции к бутстреп-выборке

```{r}

set.seed(my.seed)
boot.fn(DF.boston, sample(n, n, replace = T))

```


применяем функцию boot для вычисления стандартных ошибок параметров

```{r}
 
#  (1000 выборок с повторами)
boot(DF.boston, boot.fn, 1000)

```


 сравним с ошибками параметров по МНК

```{r}
# К
summary(fit.lm.1)$coef
summary(fit.lm.1_1)$coef

```


 график остатков модели

```{r}
 
plot(fit.lm.1, 3)
plot(fit.lm.1_1, 3)

```



```{r}

# вычислим оценки параметров квадратичной модели регрессии
boot.fn.2 <- function(data, index){
  coef(lm(crim ~ poly(tax , 2) + poly(indus, 2) +  poly(black, 2), data = data, subset = index))
}
# применим функцию к 1000 бутсреп-выборкам
set.seed(my.seed)
boot(DF.boston, boot.fn.2, 1000)

```

сравним с ошибками параметров по МНК

```{r}

summary(fit.lm.2)$coef
summary(fit.lm.2_1)$coef

```


график остатков модели

```{r}

plot(fit.lm.2, 3)
plot(fit.lm.2_1, 3)

```

Нелинейность в остатках полинома третьей степени остаётся, и бутстреп-ошибки параметров модели выше, чем аналогичные МНК-оценки. 

При сопоставлении ошибок параметров, полученных с помощью МНК и бутстрепом заметим, что они достаточо близки, но не эдентичны.